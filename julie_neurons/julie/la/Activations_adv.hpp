/******************************************************************************
 *             Copyright 2020 DeepFrame AI
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 ******************************************************************************/

#pragma once
#include "iMatrix.hpp"
#include "Activations_CPU_adv.hpp"
#ifdef WITH_CUDA
#include "Activations_CUDA_adv.hpp"
#endif

namespace julie
{
namespace la
{

/*********************************************************************************
 * This is the base class of all raw activation functions that hold one trainable
 * scalar parameter. For example: PReLU, Swish, etc. Activation functions
 * like ReLU or Sigmoid that do not hold trainable parameters will not derive from
 * this base class.
 *********************************************************************************/
template <typename DT>
class Activation_adv
{
public:

    // Default constructor
    Activation_adv();

    // Forward operation of the activation giving the activation's derivatives
    // Arguments:
    //     output:     Output of this activation function
    //     diff:       The derivative d(output) / d(input) generated by this activation function
    //     alpha_diff: The derivative d(output) / d(alpha) generated by this activation function
    //     in:         Input of this activation function
    //     alpha:      The trainable scalar input of this activation function
    // Returns: void
    void operator () (iMatrix<DT> & output, iMatrix<DT> & diff, iMatrix<DT> &alpha_diff, const iMatrix<DT> & in, const DT &alpha);

    // Forward operation of the activation without providing the activation's derivatives
    // Arguments:
    //     output: Output of this activation function
    //     in:     Input of this activation function
    //     alpha:  The trainable scalar input of this activation function
    // Returns:    void
    void operator () (iMatrix<DT> & output, const iMatrix<DT> & in, const DT &alpha);

protected:

    // Reference to the naked CPU type activation function.
    // This reference will be instantiated by derived classes (PReLU, Swish, etc).
    std::shared_ptr<cpu::Activation_CPU_adv<DT>> m_act_cpu;
#ifdef WITH_CUDA
    // Reference to the naked CUDA type activation function.
    // This reference will be instantiated by derived classes (PReLU, Swish, etc).
    std::shared_ptr<cuda::Activation_CUDA_adv<DT>> m_act_cuda;
#endif
};

template <typename DT>
class PReLU : public Activation_adv<DT>
{
public:
    PReLU();
};

}  // namespace la
}  // namespace julie

