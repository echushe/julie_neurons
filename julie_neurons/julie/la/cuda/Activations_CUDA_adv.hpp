/******************************************************************************
 *             Copyright 2020 DeepFrame AI
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 ******************************************************************************/

#pragma once
#include "Matrix_CUDA.hpp"

namespace julie
{
namespace la
{
namespace cuda
{

/*********************************************************************************
 * This is the base class of all CUDA mode activation functions that hold one
 * trainable scalar parameter. For example: PReLU, Swish, etc. Activation functions
 * like ReLU or Sigmoid that do not hold trainable parameters will not derive from
 * this base class.
 *********************************************************************************/
template <typename DT>
class Activation_CUDA_adv
{
public:

    // Default constructor
    Activation_CUDA_adv() {}

    // Forward operation of the activation giving the activation's derivatives
    // Arguments:
    //     output:     Output of this activation function
    //     diff:       The derivative d(output) / d(input) generated by this activation function
    //     alpha_diff: The derivative d(output) / d(alpha) generated by this activation function
    //     in:         Input of this activation function
    //     alpha:      The trainable scalar input of this activation function
    // Returns: void
    virtual void operator () (Matrix_CUDA<DT> & output, Matrix_CUDA<DT> & diff, Matrix_CUDA<DT> & alpha_diff, const Matrix_CUDA<DT> & in, const DT & alpha) = 0;

    // Forward operation of the activation without providing the activation's derivatives
    // Arguments:
    //     output: Output of this activation function
    //     in:     Input of this activation function
    //     alpha:  The trainable scalar input of this activation function
    // Returns:    void
    virtual void operator () (Matrix_CUDA<DT> & output, const Matrix_CUDA<DT> & in, const DT & alpha) = 0;
};

template <typename DT>
class PReLU : public Activation_CUDA_adv<DT>
{
public:

    // Forward operation of the activation giving the activation's derivatives
    // Arguments:
    //     output:     Output of this activation function
    //     diff:       The derivative d(output) / d(input) generated by this activation function
    //     alpha_diff: The derivative d(output) / d(alpha) generated by this activation function
    //     in:         Input of this activation function
    //     alpha:      The trainable scalar input of this activation function
    // Returns: void
    void operator () (Matrix_CUDA<DT> & output, Matrix_CUDA<DT> & diff, Matrix_CUDA<DT> & alpha_diff, const Matrix_CUDA<DT> & in, const DT & alpha);

    // Forward operation of the activation without providing the activation's derivatives
    // Arguments:
    //     output: Output of this activation function
    //     in:     Input of this activation function
    //     alpha:  The trainable scalar input of this activation function
    // Returns:    void
    void operator () (Matrix_CUDA<DT> & output, const Matrix_CUDA<DT> & in, const DT & alpha);
};


} // namespace cuda
} // namespace la
} // namespace julie